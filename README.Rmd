---
title: "Project 2"
author: "Ipsita Datta & Victoria Seng"
date: "10/15/2021"
params:
  channel: data_channel_is_bus
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, fig.path = "./images/")
```

# Predicting News Article Popularity

## Introduction

This data set contains a selection of news articles in six different channels, along with attributes about those articles. Our goal is to fit and test the best model using the variables provided to predict the response, number of shares, for future articles. 

Predictors include day of the week on which the article was published, keywords in the title and body, videos and images in the article, polarity (positive and negative) of the title and content, and more. 

We will do some exploratory analysis of the variables and fit four models: two linear regression models, and two ensemble tree-based models, a random forest and a boosted tree. 

## Data

```{r libs, echo = FALSE}
library(tidyverse)
library(caret)
library(randomForest)
library(gbm)
library(corrplot)
library(plotrix)
set.seed(300)
```

First, we import the news popularity data csv. We'll also pull the channel names for later use.

```{r data_import_subset}
newsPop <- read_csv("./Data/OnlineNewsPopularity.csv")

channel <- newsPop %>%
  select(starts_with("data_channel_is_")) %>%
    names

```

Next, we subset the channel for use in the initial EDA and model fitting. 
<<<<<<< HEAD

NOTE: Ipsita, this is very similar to your data transformation for the pie chart, but I didn't substring the channel names.
We can edit to use just one of these but I'm putting this here for now.

=======

NOTE: Ipsita, this is very similar to your data transformation for the pie chart, but I didn't substring the channel names.
We can edit to use just one of these but I'm putting this here for now.

>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b
```{r transpose_channels}
newsTP <- newsPop %>%
  pivot_longer(starts_with("data_channel_is_"), names_to = "chname", values_to = "v") %>%
    filter(v == TRUE) %>%
      mutate(channel = chname) %>%
        select(-chname, -v) 
```

```{r subset}
subNews <- newsTP %>%
  filter(channel == params$channel) %>%
    select(-starts_with("data_channel_is_"), -url, -timedelta, -channel)
```

## Summaries

<<<<<<< HEAD
For the first few summaries, we will use this transposed data that turns the binary weekday variables into one categorical variable.
=======
For the first few summaries, we will use this transposed data that turns the binary weekday variables into one categorical varible.
>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b

```{r transpose_weekdays}
TP <- subNews %>%
  pivot_longer(starts_with("weekday_is"), names_to = "wd", values_to = "wdVal") %>%
    filter(wdVal == TRUE) %>%
      mutate(weekday = substr(wd, 12, nchar(wd))) %>%
        select(-wd, -wdVal) 
```

This contingency table shows the overall number of shares by weekday. A higher number means more shares have happened on that day.
```{r weekday_contingency}
GDAtools::wtable(TP$weekday, w = TP$shares)
```

Here is the contingency table for best keywords shares by weekdays
```{r weekday_contingency_bestkeyword_shares}
GDAtools::wtable(TP$weekday, w = TP$kw_max_max)
```

Here is contingency table for the number of maximum shares of an article that
was linked in the article by weekday
```{r contingency_self_reference_max}
GDAtools::wtable(TP$weekday, w = TP$self_reference_max_shares)
```

Here are some summary stats by weekday. Again, bigger means more shares.
```{r weekday_summaries}
TP %>%
  group_by(weekday) %>%
    summarize(min = min(shares), mean = mean(shares), median = median(shares), iqr = IQR(shares), max = max(shares))
```

A higher number  means  more best keywords tend to be shared more often by weekday
```{r weekday_summaries_bestKeyword}
TP %>%
  group_by(weekday) %>%
    summarize(min = min(kw_max_max), mean = mean(kw_max_max), median = median(kw_max_max), stddev=sd(kw_max_max),iqr = IQR(kw_max_max), max = max(kw_max_max))
```

Transpose newsPop data to create pie chart across all the data channels
```{r newsPop_TP}
newsPop_TP <- newsPop %>%
  pivot_longer(starts_with("data_channel_is_"), names_to = "dc", values_to = "dcVal") %>%
    filter(dcVal == TRUE) %>%
      mutate(datachannel = substr(dc, 17, nchar(dc))) %>%
        select(datachannel,shares,-dc, -dcVal) 
```

Here is the pie chart showing overall which datachannel is doing best
```{r piechart}
df<-newsPop_TP %>% group_by(datachannel) %>% summarise_each(funs(sum))
pct <- round(df$shares/sum(df$shares)*100)
lbls <- paste(df$datachannel, pct) # add percents to labels
lbs<-paste(lbls,"%",sep="")
pie3D(df$shares,labels=lbs,explode=0.1,
   main="Pie Chart of Shares across datachannels ")
```

Here is the bar plot showing bestkeyword shares by weekdays.Heigher means
for bestkeyword shares on that day
```{r bestkeyword_bar}
plot1<-ggplot(TP,aes(x=weekday,y=kw_max_max,fill=weekday))
plot1+ geom_col() + 
  scale_x_discrete("weekday") + 
  ggtitle("bestkeyword shares across weekdays")+
  scale_y_continuous(labels = scales::comma)
```

```{r share_weekdays_bar}
plot2<-ggplot(TP,aes(x=weekday,y=shares,fill=weekday))
plot2+ geom_col() + 
  scale_x_discrete("weekday") + 
  ggtitle(" shares across weekdays")+
  scale_y_continuous(labels = scales::comma)
```

Here is a boxplot with jitter of shares by weekend status. The lines on the box plot show quartile values by weekend status. 
```{r weekend_box}
g <- ggplot(data = subNews, aes(x = as_factor(is_weekend), y = shares, group = as_factor(is_weekend)))
g + geom_jitter(aes(color = as_factor(is_weekend))) +
  geom_boxplot() +
      labs(x = "Weekend Status (1 = Weekend, 0 = Weekday)", 
           y = "Number of Shares", 
           title = "Box Plot with Jitter of Shares by Weekend Status") +
        scale_y_continuous(labels = scales::comma, expand = expansion(mult = c(0,0)))
```

Here is a scatter plot of shares vs positive word rate with a grouping on weekday/weekend. The Y axis represents shares. More points higher on the Y axis on the lefthand side of the graph indicate less positive articles getting more shares. More points higher on the Y axis on the righthand side of the graph indicate more positive articles getting more shares. The prevelence of one color over another in any part of the graph is indicative of whether the article was published on a weekday or a weekend.

```{r pos_v_share}
g <- ggplot(data = subNews, aes(rate_positive_words, shares))
g + geom_point(aes(color = as_factor(is_weekend)), position = "jitter") + 
  labs(x = "Rate Positive Words", 
       y = "Number of Shares", 
       title = "Positive Word Rate vs Number of Shares by Weekday Status",
       color = "Weekend? (1 = Yes, 0 = No)") + 
    scale_y_continuous(labels = scales::comma)
```

Here is the scatterplot showing number of images vs shares.
From the graph , we can say less number of images have more shares.
Specifically those articles has zero or one image.

```{r images_v_shares}
g <- ggplot(data = subNews, aes(num_imgs, shares))
g + geom_point( position = "jitter") + 
  labs(x = "num_imgs", 
       y = "Number of Shares", 
       title = "num_imgs vs Number of Shares "
       ) + 
    scale_y_continuous(labels = scales::comma)
```

Here is a scatter plot of shares vs negative word rate with a grouping on weekday/weekend. The Y axis represents shares. More points higher on the Y axis on the lefthand side of the graph indicate less negative articles getting more shares. Less points higher on the Y axis on the righthand side of the graph indicate more negative articles getting less shares. The prevelence of one color over another in any part of the graph is indicative of whether the article was published on a weekday or a weekend.

```{r negative_words_V_shares_scatterplot}
g1 <- ggplot(data = subNews, aes(rate_negative_words, shares))
g1 + geom_point(aes(color = as_factor(is_weekend)), position = "jitter") + 
  labs(x = "negative words rate", 
       y = "Number of Shares", 
       title = "negative words rate vs Number of Shares by Weekend Status",
       color = "Weekend? (1 = Yes, 0 = No)") + 
    scale_y_continuous(labels = scales::comma) 
```

<<<<<<< HEAD
Here is the scatter plot showing points in the middle has more share as a function of number of words in the title. On Left of the graph with less word in the title has less share by weekdays
=======
Here is the scatter plot showing poins in the middle has more share as a function of number of words in the title. On Left of the graph with less word in the 
title has less share by weekdays
>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b

```{r wordsIntitle}
g2 <- ggplot(data = subNews, aes(n_tokens_title, shares))
g2 + geom_point(aes(color = as_factor(is_weekend)), position = "jitter") + 
  labs(x = "Number of words in the title", 
       y = "Number of Shares", 
       title = "Number of words in the title vs Number of Shares by Weekend Status",
       color = "Weekend? (1 = Yes, 0 = No)") + 
    scale_y_continuous(labels = scales::comma) 
```
<<<<<<< HEAD
=======

To find correlation coefficient between shares and other variable,subsetting
dataset with only numeric variables

```{r TP_numeric_correlation}
bus_numeric<-dplyr::select_if(subNews,is.numeric)
>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b


This correlation plot visually shows the positive or negative correlation coefficient between the various predictor varibles. The larger the circle and more saturated the color, the stronger the correlation. Blue is negative, and red is positive.

```{r corrplot}
c <- cor(subNews %>% select(-shares))
cp <- corrplot(c, order = 'AOE', cl.pos = 'n', tl.cex = 0.5)
```

<<<<<<< HEAD
## Model Fitting

A linear regression model describes the relationship between a dependent variable and one or more independent variables, i.e y=beta0+beta1 * x1+beta2 * x2 +....+betan * xn where y is dependent response variable and x1 to xn are independent variable (also called predictors)

Linear models are a way of describing a response variable in terms of a linear combination of predictor variables.  

splitting data in train(70%) and test (30%)
=======
This correlation plot visually shows the positive or negative correlation coefficient between the various predictor varibles. The larger the circle and more saturated the color, the stronger the correlation. Blue is negative, and red is positive.

```{r corrplot}
c <- cor(subNews %>% select(-shares))
cp <- corrplot(c, order = 'AOE', cl.pos = 'n', tl.cex = 0.5)
```

## Model Fitting

A linear regression model describes the relationship between a dependent variable and one or more independent variables, i.e y=beta0+beta1 * x1+beta2 * x2 +....+betan * xn where y is dependent response variable and x1 to xn are independent variable (also called predictors)

Linear models are a way of describing a response variable in terms of a linear combination of predictor variables.  

splitting data in train(70%) and test (30%)

```{r split_data}
Index <- createDataPartition(y = subNews$shares , p= 0.7, list = FALSE)
Train <- subNews[Index,]
Test <- subNews[-Index,]
```

### First Linear Model
>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b

```{r split_data}
Index <- createDataPartition(y = subNews$shares , p= 0.7, list = FALSE)
Train <- subNews[Index,]
Test <- subNews[-Index,]
```

### First Linear Model
Based on the p-values and correlation coefficients  , the predictors are added to
the model to predict shares.
```{r Linear_Regression,echo=FALSE}

lmfit<-train(shares ~ num_hrefs+kw_max_avg+weekday_is_saturday+weekday_is_sunday+
<<<<<<< HEAD
title_subjectivity+global_sentiment_polarity+LDA_01+LDA_04+rate_positive_words+
num_imgs+num_videos,data = Train, method = "lm", 
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv", number = 10))


#Performance on Test data set for Linear Regression model

=======
+title_subjectivity+global_sentiment_polarity+LDA_01+LDA_04+rate_positive_words+
num_imgs+num_videos,
             data = Train, 
        method = "lm", 
       preProcess = c("center", "scale"),
      trControl = trainControl(method = "cv", number = 10))
```

Performance on Test data set for Linear Regression model
```{r lm_Test}
>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b
pred <- predict(lmfit, newdata =Test)
lm_Test<-round(postResample(pred, obs = Test$shares),4)
```

### Second Linear Model

This linear regression model chooses its predictors based on excluding any variable that has a correlation coefficient of above 0.3 or below -0.3 with any other variable. This should choose variables for each channel.

```{r select_vars}
cTib <- as_tibble(c) %>%
  mutate(v1 = names(as_tibble(c))) %>%
    pivot_longer(cols = !v1, names_to = "v2", values_to = "corr") %>%
      filter(abs(corr) > 0.3 & v1 != v2)

exclude <- distinct(cTib, v1) %>% 
  unlist() %>% 
    unname()

vChan <- subNews %>%
  select(-all_of(exclude))

vIndex <- createDataPartition(y =vChan$shares , p= 0.7, list = FALSE)
vTrain <- vChan[vIndex,]
vTest <- vChan[-vIndex,]

```

<<<<<<< HEAD
=======
### Second Linear Model

This linear regression model chooses its predictors based on excluding any variable that has a correlation coefficient of above 0.3 or below -0.3 with any other variable. This should choose varibles for each channel.

```{r select_vars}
cTib <- as_tibble(c) %>%
  mutate(v1 = names(as_tibble(c))) %>%
    pivot_longer(cols = !v1, names_to = "v2", values_to = "corr") %>%
      filter(abs(corr) > 0.3 & v1 != v2)

exclude <- distinct(cTib, v1) %>% 
  unlist() %>% 
    unname()

vChan <- subNews %>%
  select(-all_of(exclude))

vIndex <- createDataPartition(y =vChan$shares , p= 0.7, list = FALSE)
vTrain <- vChan[vIndex,]
vTest <- vChan[-vIndex,]

```

>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b
This is the model. It is pre-processed with centering and scaling and it uses 10-fold CV. The variables involved may vary from channel to channel.

```{r v_linreg}
vMod <- train(
  shares ~ .,
  data = vTrain,
  method = "lm",
  preProcess = c("center","scale"),
  trControl = trainControl(method = "cv", number = 10)
)

vPred <- predict(vMod, newdata = vTest)
<<<<<<< HEAD
v_linreg<-round(postResample(vPred, obs = vTest$shares), 4)
=======
round(postResample(vPred, obs = vTest$shares), 4)
>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b
```

### Ensemble tree-based modeling

<<<<<<< HEAD
### Random Forest model
 A random forest model is a supervised learning algorithm which extend the idea
 of bagging method  to solve regression or classification problems. It creates
 multiple trees from bootstrap samples like as bagging method and then use a
 random subset of predictors for each bootstrap sample/tree fit. It doesn't use 
 all the predictor.
 
```{r ,cache=TRUE}
randomforestFit <- train(
               shares ~ .,
               data = Train,
               method = "rf",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv",number = 5),
               tuneGrid = data.frame(mtry = seq(1,10,1)))

```
# Performance on Test data set for Randomforest model
```{r final_model}
pred <- predict(randomforestFit, newdata =Test)
rf_test<-round(postResample(pred, obs = Test$shares),4)
```


### Boosted tree model

```{r vBoostTree, results = FALSE}
vBoost <- train(
  shares ~ ., 
  data = Train,
  method = "gbm",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "repeatedcv", number = 5, repeats = 3)
)
```
=======
To do an ensemble tree-based model,we created a new column called popularity and add it to the trsining and test dataset. If the number of shares is greater than 2500, then popularity is equal to 1 (popular) , otherwise 0 (unpopular).

```{r add_binary_column}
Train_glm<- Train %>% 
            mutate(popularity = if_else((shares >= 1500),1,0)) %>%
            select(-shares)
Test_glm<- Test %>%
          mutate(popularity = if_else((shares >= 1500),1,0)) %>%
          select(-shares) 
```

### Boosted tree model

```{r vBoostTree, results = FALSE}
vBoost <- train(
  shares ~ ., 
  data = Train,
  method = "gbm",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "repeatedcv", number = 5, repeats = 3)
)
```
>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b

## Comparison
We have created 4 models - 2 linear regression model,1 Random Forest Model and 1
Boosted tree model. Here in this given data set, we are trying to predict the 
number of shares of a news article .
We are comparing four model with respect to their RMSE.
```{r comparison}
# need to add boosted RMSE on test
tab<-matrix(round(c(lm_Test[1],v_linreg[1],rf_test[1]),1))
 
colnames(tab) <- c('RMSE')
rownames(tab) <- c('LM1','LM2','RandomForest')
print(tab)


```



<<<<<<< HEAD
=======
ID
>>>>>>> fb6636cc7506c3de379896ee991a9fedadc2656b

